import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    roc_curve,
    auc
)
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# As vari√°veis X_train_scaled, X_test_scaled, Y_train e Y_test j√° foram preparadas
# na c√©lula anterior. Reutilizaremos essas vari√°veis para evitar processamento redundante.
# Renomeando para X_train e X_test para consist√™ncia com o loop de treinamento:
X_train = X_train_scaled
X_test = X_test_scaled

# --- 2. TREINAMENTO DOS MODELOS ---

models = {}
# Adicionar probability=True para SVC para permitir c√°lculo de AUC
models['LogisticRegression'] = LogisticRegression(max_iter=500, random_state=42)
models['SVC_Linear'] = SVC(kernel='linear', random_state=42, probability=True)
models['SVC_Poly_Degree_2'] = SVC(kernel='poly', degree=2, random_state=42, probability=True)
models['SVC_Poly_Degree_3'] = SVC(kernel='poly', degree=3, random_state=42, probability=True)

print("--- üß† Treinamento dos Modelos Iniciado ---")
for name, model in models.items():
    model.fit(X_train, Y_train)
    print(f"‚úÖ Modelo {name} treinado.")
print("-" * 50)


# --- 3. AVALIA√á√ÉO DOS MODELOS (Etapa 4 + Opcional: ROC/AUC) ---

all_metrics = []
plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], 'k--', label='Aleat√≥rio (AUC = 0.50)')

for name, model in models.items():
    # Previs√£o das classes e das probabilidades
    y_pred = model.predict(X_test)

    # y_proba √© crucial para ROC/AUC: prever a probabilidade da classe 1 (Maligno)
    # LogisticRegression e SVC com probability=True t√™m o m√©todo predict_proba
    y_proba = model.predict_proba(X_test)[:, 1]

    # --- M√©tricas B√°sicas ---
    cm = confusion_matrix(Y_test, y_pred)
    report = classification_report(Y_test, y_pred, target_names=['Benigno (0)', 'Maligno (1)'], output_dict=True)

    acc = accuracy_score(Y_test, y_pred)
    prec = report['Maligno (1)']['precision']
    rec = report['Maligno (1)']['recall']
    f1 = report['Maligno (1)']['f1-score']

    # --- Curva ROC e AUC (Opcional) ---
    fpr, tpr, thresholds = roc_curve(Y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    # Plotar a Curva ROC
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})')

    # Armazenar para a tabela comparativa
    all_metrics.append({
        'Modelo': name,
        'Acur√°cia': acc,
        'Precis√£o': prec,
        'Recall': rec,
        'F1-Score': f1,
        'AUC': roc_auc
    })

    # Impress√£o da Matriz de Confus√£o e M√©tricas
    print(f"\n\n======== üìä Avalia√ß√£o Detalhada: {name} ========")
    cm_df = pd.DataFrame(cm, index=['Real B (0)', 'Real M (1)'], columns=['Previsto B (0)', 'Previsto M (1)'])
    print("\n1. Matriz de Confus√£o:")
    print(cm_df.to_markdown())
    print(f"\n2. Acur√°cia: {acc:.4f}")
    print(f"3. Precis√£o (Maligno/1): {prec:.4f}")
    print(f"4. Recall (Maligno/1): {rec:.4f}")
    print(f"5. F1-score (Maligno/1): {f1:.4f}")
    print(f"6. AUC (Area Under the Curve): {roc_auc:.4f}")


# --- 4. Tabela Comparativa de Desempenho (Incluindo AUC) ---

print("\n\n" + "=" * 80)
print("             üèÜ Tabela Comparativa de Desempenho (Acur√°cia, Precis√£o, Recall, F1, AUC)")
print("=" * 80)

results_df = pd.DataFrame(all_metrics)
results_df = results_df.sort_values(by='AUC', ascending=False).reset_index(drop=True)

print(results_df.to_markdown(index=True, floatfmt=".4f"))


# --- 5. Visualiza√ß√£o da Curva ROC ---
plt.xlabel('Taxa de Falso Positivo (FPR)')
plt.ylabel('Taxa de Verdadeiro Positivo (TPR)')
plt.title('Curva ROC Comparativa dos Modelos')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print("\n**Nota:** A AUC mede a capacidade de distin√ß√£o do modelo, onde 1.0 √© perfeito.")
